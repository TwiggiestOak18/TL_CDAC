{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07ESpmYwpoUF",
    "outputId": "c8a39ccf-1f92-4309-e946-9c8da7e10268"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4GEDic-vh2T",
    "outputId": "612fe777-78fc-4ce6-8297-3c1c07f73fe7"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "jxMPEB5gwE_-",
    "outputId": "e6e89aec-9ce6-49ce-9842-e4b5de6e416e"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(\"transliteration.txt\",delimiter = \"\\t\",header=None,encoding='utf-8',na_filter = False)\n",
    "\n",
    "\n",
    "X = dataset.iloc[:,0]\n",
    "y = dataset.iloc[:,-1]\n",
    "\n",
    "\n",
    "import Data_preprocessing\n",
    "\n",
    "\n",
    "\n",
    "source_int_text, target_int_text, source_vocab_to_int, target_vocab_to_int,source_int_to_vocab,target_int_to_vocab = Data_preprocessing.preprocess(X,y)\n",
    "\n",
    "\n",
    "import Layers\n",
    "import Model_Inputs\n",
    "\n",
    "\n",
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  target_sequence_length,\n",
    "                  max_target_word_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "\n",
    "\n",
    "    enc_outputs, enc_states = Layers.encoding_layer(input_data, \n",
    "                                             rnn_size, \n",
    "                                             num_layers, \n",
    "                                             keep_prob, \n",
    "                                             source_vocab_size, \n",
    "                                             enc_embedding_size)\n",
    "    \n",
    "    dec_input = Model_Inputs.process_decoder_input(target_data, \n",
    "                                      target_vocab_to_int, \n",
    "                                      batch_size)\n",
    "    \n",
    "    train_output, infer_output = Layers.decoding_layer(dec_input,\n",
    "                                               enc_states, \n",
    "                                               target_sequence_length, \n",
    "                                               max_target_word_length,\n",
    "                                               rnn_size,\n",
    "                                              num_layers,\n",
    "                                              target_vocab_to_int,\n",
    "                                              target_vocab_size,\n",
    "                                              batch_size,\n",
    "                                              keep_prob,\n",
    "                                              dec_embedding_size)\n",
    "    \n",
    "    return train_output, infer_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display_step = 200\n",
    "\n",
    "epochs = 60\n",
    "batch_size = 30\n",
    "\n",
    "rnn_size = 64\n",
    "num_layers = 2\n",
    "\n",
    "encoding_embedding_size = 50\n",
    "decoding_embedding_size = 50\n",
    "\n",
    "learning_rate = 0.001\n",
    "keep_probability = 0.5\n",
    "\n",
    "\n",
    "save_path = 'checkpoints/dev'\n",
    "\n",
    "\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "\n",
    "    input_data, targets, target_sequence_length, max_target_sequence_length = Model_Inputs.enc_dec_model_inputs()\n",
    "    lr, keep_prob = Model_Inputs.hyperparam_inputs()\n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int)\n",
    "    \n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        \n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]\n",
    "\n",
    "\n",
    "import Batch_Metrics\n",
    "\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(Batch_Metrics.get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             source_vocab_to_int['<PAD>'],\n",
    "                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n",
    "\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                Batch_Metrics.get_batches(train_source, train_target, batch_size,\n",
    "                            source_vocab_to_int['<PAD>'],\n",
    "                            target_vocab_to_int['<PAD>'])):\n",
    "\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "        \n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                train_acc = Batch_Metrics.get_accuracy(target_batch, batch_train_logits)\n",
    "                valid_acc = Batch_Metrics.get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "                print('Epoch {:>3} Batch {:>3}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i+1, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and saved')\n",
    "    \n",
    "  \n",
    "def save_params(params):\n",
    "    with open('params.p', 'wb') as out_file:\n",
    "        pickle.dump(params, out_file)\n",
    "\n",
    "save_params(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5ptUlZkwIgo"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import Data_preprocessing\n",
    "\n",
    "def load_params():\n",
    "    with open('params.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = Data_preprocessing.load_preprocess()\n",
    "\n",
    "load_path = load_params()\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 30\n",
    "\n",
    "def word_to_seq(word, vocab_to_int):\n",
    "    results = []\n",
    "    for word in list(word):\n",
    "        if word in vocab_to_int:\n",
    "            results.append(vocab_to_int[word])\n",
    "        else:\n",
    "            results.append(vocab_to_int['<UNK>'])\n",
    "            \n",
    "    return results\n",
    "\n",
    "print(\"\\n Enter word to be transliterated:\")\n",
    "transliterate_word = input().lower()\n",
    "\n",
    "\n",
    "\n",
    "transliterate_word = word_to_seq(transliterate_word, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "        \n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    \n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    transliterate_logits = sess.run(logits, {input_data: [transliterate_word]*batch_size,\n",
    "                                         target_sequence_length: [len(transliterate_word)]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in transliterate_word]))\n",
    "print('  English Word: {}'.format([source_int_to_vocab[i] for i in transliterate_word]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Id:      {}'.format([i for i in transliterate_logits]))\n",
    "\n",
    "output = \"\"\n",
    "for i in transliterate_logits:\n",
    "        if target_int_to_vocab[i]!= '<EOS>':\n",
    "                output = output + target_int_to_vocab[i]\n",
    "print('  Hindi Word:      {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CDAC_TL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
